{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\ayman\\\\Downloads\\\\MSU_Thesis-master\\\\MSU_Thesis-master\\\\dataset\\\\trip_advisor_dataset\\\\combined\\\\combined/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5d8c6f252dbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontainer_path_comb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m \u001b[0mfull_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_unbalance_data_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontainer_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruthful_percentage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeceptive_percentage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5d8c6f252dbe>\u001b[0m in \u001b[0;36mget_unbalance_data_set\u001b[1;34m(container_path, truthful_percentage, deceptive_percentage)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_unbalance_data_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontainer_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruthful_percentage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeceptive_percentage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     training_data = load_files(container_path, description=None, load_content=True,\n\u001b[1;32m---> 52\u001b[1;33m                                shuffle=True, encoding='ISO-8859-1', decode_error='strict', random_state=0)\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mfilter_truthful_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPUFINAL\\lib\\site-packages\\sklearn\\datasets\\_base.py\u001b[0m in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     folders = [f for f in sorted(listdir(container_path))\n\u001b[0m\u001b[0;32m    164\u001b[0m                if isdir(join(container_path, f))]\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\ayman\\\\Downloads\\\\MSU_Thesis-master\\\\MSU_Thesis-master\\\\dataset\\\\trip_advisor_dataset\\\\combined\\\\combined/'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import  random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation ,Dropout, Conv1D, Conv2D, MaxPooling1D, Flatten, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def get_unbalance_data_set(container_path, truthful_percentage, deceptive_percentage):\n",
    "    training_data = load_files(container_path, description=None, load_content=True,\n",
    "                               shuffle=True, encoding='ISO-8859-1', decode_error='strict', random_state=0)\n",
    "\n",
    "    filter_truthful_index = []\n",
    "    filter_deceptive_index = []\n",
    "    for index in range(0, len(training_data.data)):\n",
    "        if (training_data.target[index] == True):\n",
    "            filter_truthful_index.append(index)\n",
    "        else:\n",
    "            filter_deceptive_index.append(index)\n",
    "\n",
    "    filter_data_truthful = random.sample(filter_truthful_index, (int)(len(training_data.data)/2 * truthful_percentage))\n",
    "    filter_data_deceptive = random.sample(filter_deceptive_index, (int)(len(training_data.data)/2 * deceptive_percentage))\n",
    "\n",
    "    filter_data = filter_data_truthful+filter_data_deceptive\n",
    "\n",
    "    list_need_to_delete = []\n",
    "    new_data_list = []\n",
    "\n",
    "    for index in range(0, len(training_data.data)):\n",
    "        if index not in filter_data:\n",
    "\n",
    "            list_need_to_delete.append(index)\n",
    "        else:\n",
    "            new_data_list.append(training_data.data[index])\n",
    "    training_data.target = np.delete(training_data.target, list_need_to_delete)\n",
    "    training_data.data =  new_data_list\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def words_tag_freq_calculation(container_path, truthful):\n",
    "\n",
    "    #load data set from given directory path\n",
    "    training_data = load_files(container_path, description=None,  load_content=True,\n",
    "                              shuffle=True, encoding='ISO-8859-1', decode_error='strict', random_state=0)\n",
    "\n",
    "    filter_data = []\n",
    "    for index in range(0, len(training_data.data)) :\n",
    "        if(training_data.target[index] == truthful):\n",
    "            #print('Target: ', training_data.target[index], 'Content: ', training_data.data[index])\n",
    "            filter_data.append(training_data.data[index])\n",
    "\n",
    "    return filter_data\n",
    "\n",
    "def topic_word_distribution(topic_word_list, container_path):\n",
    "    training_data = load_files(container_path, description=None, load_content=True,\n",
    "                               shuffle=True, encoding='ISO-8859-1', decode_error='strict', random_state=0)\n",
    "\n",
    "    filter_data = []\n",
    "    for each_word in topic_word_list:\n",
    "        count_T = 0\n",
    "        count_F = 0\n",
    "        for index in range(0, len(training_data.data)):\n",
    "            # print('Target: ', training_data.target[index], 'Content: ', training_data.data[index])\n",
    "            str_data = training_data.data[index]\n",
    "            if str_data.find(each_word) != -1 :\n",
    "                if (training_data.target[index] == True):\n",
    "                    count_T += 1\n",
    "                else:\n",
    "                    count_F += 1\n",
    "\n",
    "        print(each_word, \" appears in Truthful review : \", count_T, \" and Deceptive review : \", count_F)\n",
    "\n",
    "def get_data_sentence_containing_topic_model_words(container_path, topic_word_list, data):\n",
    "    training_data = data\n",
    "    if (training_data == None):\n",
    "        training_data = load_files(container_path, description=None, load_content=True,\n",
    "                               shuffle=True, encoding='ISO-8859-1', decode_error='strict', random_state=0)\n",
    "    filter_data = []\n",
    "    #print(training_data.data[2])\n",
    "    for index in range(0, len(training_data.data)):\n",
    "        #print(\"Before: \", training_data.data[index])\n",
    "        document = sent_tokenize(training_data.data[index])\n",
    "        new_document = \"\"\n",
    "        for sentence in document:\n",
    "            word_list = word_tokenize(sentence.lower())\n",
    "            for word in topic_word_list:\n",
    "                if word.lower() in word_list:\n",
    "                    new_document += sentence+\" \"\n",
    "                    break\n",
    "\n",
    "        training_data.data[index] = new_document\n",
    "        #print(\"After: \", new_document)\n",
    "\n",
    "    #print(training_data.data[2])\n",
    "    return training_data\n",
    "\n",
    "def get_lemmatize_data_set(container_path):\n",
    "    training_data = load_files(container_path, description=None, load_content=True,\n",
    "                               shuffle=True, encoding='ISO-8859-1', decode_error='strict', random_state=0)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for index in range(0, len(training_data.data)):\n",
    "        # print(\"Before: \", training_data.data[index])\n",
    "        #document = word_tokenize(training_data.data[index])\n",
    "        #print(training_data.data[index])\n",
    "        word_list = word_tokenize(training_data.data[index])\n",
    "        new_document = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list])\n",
    "       # print(new_document)\n",
    "\n",
    "        training_data.data[index] = new_document\n",
    "\n",
    "    return training_data\n",
    "\n",
    "def print_top_words(container_path, model, feature_names, n_top_words):\n",
    "    full_str = \"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        full_str += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        full_str += \" \"\n",
    "        str_topic_word = \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        #topic_word_list = str_topic_word.split()\n",
    "        #topic_word_distribution(topic_word_list, container_path)\n",
    "        print(message)\n",
    "\n",
    "    print()\n",
    "    full_str = full_str.split()\n",
    "    return full_str\n",
    "\n",
    "\n",
    "def Remove(duplicate, fullRemove):\n",
    "    final_list = []\n",
    "    for num in duplicate:\n",
    "        if num not in final_list:\n",
    "            final_list.append(num)\n",
    "        else:\n",
    "            final_list.remove(num)\n",
    "            if fullRemove == False:\n",
    "                final_list.append(num)\n",
    "    return final_list\n",
    "\n",
    "def lemmatize_word_list(wordList):\n",
    "    lemmatize_list = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in wordList:\n",
    "        lemmatize_list.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "    return lemmatize_list\n",
    "\n",
    "\n",
    "def get_top_words_from_topic_modeling(container_path, n_topic, n_top_words, data):\n",
    "    training_data = data\n",
    "    if(training_data == None):\n",
    "        training_data = load_files(container_path, description=None, load_content=True,\n",
    "                               shuffle=True, encoding='ISO-8859-1', decode_error='strict', random_state=0)\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tf_review = tf_vectorizer.fit_transform(training_data.data)\n",
    "\n",
    "    lda_review = LatentDirichletAllocation(n_components=n_topic, max_iter=20,\n",
    "                                               learning_method='online',\n",
    "                                               learning_offset=50.,\n",
    "                                               random_state=0)\n",
    "    lda_review.fit(tf_review)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    return print_top_words(container_path, lda_review, tf_feature_names, n_top_words)\n",
    "\n",
    "def get_top_words_from_topic_modeling_(data, n_topic, n_top_words):\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tf_review = tf_vectorizer.fit_transform(data)\n",
    "\n",
    "    lda_review = LatentDirichletAllocation(n_components=n_topic, max_iter=20,\n",
    "                                               learning_method='online',\n",
    "                                               learning_offset=50.,\n",
    "                                               random_state=0)\n",
    "    lda_review.fit(tf_review)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    return print_top_words(\"\", lda_review, tf_feature_names, n_top_words)\n",
    "\n",
    "def remove_stopwords(dat):\n",
    "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "    output_array=[]\n",
    "    for sentence in dat.data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "def punc_removal(data): \n",
    "    output_array=[]\n",
    "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "    for article in data:\n",
    "        temp=[]\n",
    "        for word in article.split():\n",
    "            temp_w=[]\n",
    "            for char in word:\n",
    "                if char not in punc:\n",
    "                    temp_w.append(''.join(char))\n",
    "            temp.append(''.join(temp_w))\n",
    "            temp.append(''.join(\" \"))\n",
    "        output_array.append(''.join(temp))\n",
    "    return output_array\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "container_path_neg = r\"C:\\Users\\ayman\\Downloads\\MSU_Thesis-master\\MSU_Thesis-master\\dataset\\trip_advisor_dataset\\negative_polarity\\negative_polarity\"\n",
    "container_path_pos = r\"C:\\Users\\ayman\\Downloads\\MSU_Thesis-master\\MSU_Thesis-master\\dataset\\trip_advisor_dataset\\positive_polarity\\positive_polarity\"\n",
    "container_path_comb = r\"C:\\Users\\ayman\\Downloads\\MSU_Thesis-master\\MSU_Thesis-master\\dataset\\trip_advisor_dataset\\combined\\combined/\"\n",
    "\n",
    "container_path_temp = \"../data/amazon/temp/\"\n",
    "\n",
    "\n",
    "categories = ['deceptive_from_MTurk', 'truthful_from_Web']\n",
    "\n",
    "n_components = 2 #was 2\n",
    "n_top_words =200 #was 200\n",
    "\n",
    " \n",
    "data_path = container_path_comb\n",
    "\n",
    "full_data = get_unbalance_data_set(container_path=data_path, truthful_percentage=1, deceptive_percentage=1)\n",
    "\n",
    "\n",
    "\n",
    "top_word_list = get_top_words_from_topic_modeling(data_path,n_components,n_top_words, full_data)\n",
    "\n",
    "top_word_list = Remove(top_word_list, False)\n",
    "\n",
    "print(\"Total top word list:\", len(top_word_list))\n",
    "\n",
    "\n",
    "full_data = get_data_sentence_containing_topic_model_words(data_path,top_word_list, full_data)\n",
    "data= remove_stopwords(full_data)\n",
    "data=punc_removal(data)\n",
    "vocabularyList = top_word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Word2vec Implementation\"\"\"\n",
    "#imports\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data import and split\"\"\"\n",
    "\n",
    "training_data = data\n",
    "print(full_data.target_names)\n",
    "\n",
    "#Attribute values for each tuple\n",
    "X = pd.DataFrame(data)\n",
    "\n",
    "#Target output for each tuple\n",
    "Y = pd.DataFrame(full_data.target)\n",
    "#making lowercase\n",
    "X[0] = X[0].str.lower()\n",
    "#checking for null values\n",
    "X.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Preprocessing\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# X= word_tokenize(X)\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "filtered_sentence = [] \n",
    "\n",
    "# for w in X[0]: \n",
    "#     if\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(word) \n",
    "\n",
    "#Creates the relevant phrases [BIGRAMS] from the list of sentences:\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [row.split() for row in X[0]]\n",
    "# filtered_sent=[]\n",
    "# for s in sent:\n",
    "#     for x in s:\n",
    "#         if x not in stop_words:\n",
    "#             filtered_sent[s].append(x)\n",
    "        \n",
    "\n",
    "# sent = filtered_sent\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000) #min count was  30 \n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "\n",
    "\n",
    "\n",
    "#split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "# print(\"-----------------------================================================================\")\n",
    "#print(filtered_sentence) \n",
    "# print(\"-----------------------================================================================\")\n",
    "# print(stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Training Model\"\"\"\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "\n",
    "\n",
    "num_words = 5000\n",
    "size=100 #was 50\n",
    "\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=50,\n",
    "                     size=size,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.00007, \n",
    "                     negative=30, ##WAS 30 which gave 88%\n",
    "                     workers=cores-1) ##alpha was 0.03, minalpha 0.0007\n",
    "t = time()\n",
    "\n",
    "clf=w2v_model.build_vocab(X_train, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "t = time()\n",
    "\n",
    "pect=w2v_model.train(X_train, total_examples=w2v_model.corpus_count, epochs=50, report_delay=1) #epoch was 50\n",
    "words=w2v_model.wv.vocab\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2v_model.corpus_count) #Checking document length (number of samples)\n",
    "print(pect)\n",
    "w2v_model.save(\"word2vec.model\")\n",
    "w2v_model.save(\"model.bin\")\n",
    "# wv = KeyedVectors.load(\"word2vec.model\", mmap='r')\n",
    "# print(w2v_model.wv[\"and\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for w in w2v_model.wv.vocab.keys():\n",
    "    embeddings_index[w] = w2v_model.wv[w]\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words= num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for x in X_train:\n",
    "    length.append(len(x))\n",
    "max_length=max(length)\n",
    "\n",
    "x_train_seq = pad_sequences(sequences, maxlen=max_length)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Scaling the data\"\"\"\n",
    "\n",
    "# scaler=StandardScaler()\n",
    "# scaler.fit(x_train_seq)\n",
    "# x_train_seq = scaler.transform(x_train_seq)\n",
    "# scaler.fit(x_val_seq)\n",
    "# x_val_seq = scaler.transform(x_val_seq)\n",
    "# \"\"\"Normalizing the data\"\"\"\n",
    "# x_train_seq=preprocessing.normalize(x_train_seq)\n",
    "# x_val_seq=preprocessing.normalize(x_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "embedding_matrix = np.zeros((num_words, size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_fold= 1  #was:10\n",
    "result ={}\n",
    "index = 1\n",
    "accuracy = 0\n",
    "for n in range(n_fold):\n",
    "    print(\"Fold \",n, \" is started...\")\n",
    "    \"\"\" 82 percent accuracy  \"\"\"\n",
    "\n",
    "    \"\"\" test size was 20 percent,three cnn layers: filters 1200 x3 and kernel size 15,15,13, maxpooling pool size= 6, dense:2-layers:250 neurons and last layer dense 1 with softmax\"\"\"\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    e =Embedding(num_words, size, weights=[embedding_matrix], trainable= True)\n",
    "    \n",
    "    model.add(e)\n",
    "#     model.add(Conv1D(filters=1200,kernel_size=15, activation='relu' ),)\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    #ORIGINAL\n",
    "    model.add(Conv1D(filters=600,kernel_size=18, activation='relu' , ),) #kernel size was 18, filters 600\n",
    "    model.add(MaxPooling1D(pool_size=6))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(filters=500, kernel_size=14, activation='relu' , ),) #kernel size was 14, filters 500\n",
    "    model.add(MaxPooling1D(pool_size=6))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(filters=400, kernel_size= 8, activation='relu', ) ,)#kernel size was 8, filters 400\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='sparse_categorical_accuracy',mode= 'max', patience=20)\n",
    "\n",
    "    # model.add(Dense(750, activation='relu'))\n",
    "#     model.add(Dense(250, activation='relu', kernel_regularizer=keras.regularizers.l2(l=0.1)))\n",
    "#     model.add(Dense(500, activation='relu' ,kernel_regularizer=keras.regularizers.l2(l=0.1)))\n",
    "\n",
    "#     model.add(Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(l=0.01) , activity_regularizer=tf.keras.regularizers.l2(0.01)) )\n",
    "    print(model.summary())\n",
    "    # model.save(\"my_model-74\")\n",
    "\n",
    "    #Training the data: Compiling and fitting\n",
    "    # opt = keras.optimizers.Nadam(learning_rate=0.0030)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer= 'Adam', metrics=[ \"sparse_categorical_accuracy\"])\n",
    "    clf = model.fit(x_train_seq, y_train, validation_split =0.3 ,epochs=50, callbacks =[callback])#15\n",
    "    print(\"Loss and test Accuracy in fold: \",n)\n",
    "    model.evaluate(x_val_seq, y_test)\n",
    "    predicted1 = model.predict(x_val_seq) \n",
    "    predicted1= np.squeeze(predicted1)\n",
    "    predicted = []\n",
    "    pred_final=[]\n",
    "    \n",
    "    \"\"\"Turning one hot encoded output to array with each sample with one valued output\"\"\"\n",
    "    for i,x in predicted1:\n",
    "        if(i>x):\n",
    "            pred_final.append([1,0])\n",
    "            predicted.append(0)\n",
    "        else:\n",
    "            pred_final.append([0,1])\n",
    "            predicted.append(1)\n",
    "    (pred_final)\n",
    "#     for i in predicted1:\n",
    "#         if i<0.50:\n",
    "#             predicted.append(0)\n",
    "#         else:\n",
    "#             predicted.append(1)\n",
    "#     print(predicted)\n",
    "    \n",
    "    y_test= np.squeeze(np.array(y_test))\n",
    "    y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "   \n",
    "    print(y_test)\n",
    "    \"\"\"Accumulating accuracy for each fold\"\"\"\n",
    "    \n",
    "    accuracy += np.count_nonzero(predicted == y_test) *100/ len(y_test)\n",
    "    \n",
    "    \"\"\"f1 score support for each fold\"\"\"\n",
    "    result[index] = precision_recall_fscore_support(y_test, predicted)\n",
    "    index = index +1\n",
    "    print(metrics.classification_report(y_test, predicted,target_names=full_data.target_names))\n",
    "\n",
    "\"\"\"calculating average scores for all the folds\"\"\"\n",
    "\n",
    "\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1_score = 0\n",
    "#Adds up each fold precesion, recallm f1_score\n",
    "for key, value in result.items():\n",
    "    #print(key, \" = \",  value)\n",
    "    precision += value[0]\n",
    "    recall += value[1]\n",
    "    f1_score += value[2]\n",
    "    \n",
    "\n",
    "avg_accuracy = accuracy/n_fold\n",
    "avg_precision = precision/n_fold\n",
    "avg_recall = recall/n_fold\n",
    "avg_f1_score = f1_score/n_fold\n",
    "\n",
    "calculated_result = {}\n",
    "calculated_result['accuracy'] = avg_accuracy\n",
    "calculated_result['precision'] = avg_precision\n",
    "calculated_result['recall'] = avg_recall\n",
    "calculated_result['f1_score'] = avg_f1_score\n",
    "#return mean of accuracy, precision, recall, f1_score\n",
    "print(calculated_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Complete results\"\"\"\n",
    "\n",
    "for i in calculated_result.items():\n",
    "    print(i[0],\":\", i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_val_seq, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model-89.06-p-1f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.save(\"my_model-80-p-10f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPUFINAL)",
   "language": "python",
   "name": "gpufinal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
